{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM+fYxG6c5W8PPWUgsb7qW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mashrur-rahman-fahim/Machine_learning_projects/blob/main/logistic_regression_crsh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid or Logistic Function\n",
        "<img align=\"left\" src=\"./C1_W3_LogisticRegression_left.png\"     style=\" width:300px; padding: 10px; \" >As discussed in the lecture videos, for a classification task, we can start by using our linear regression model, $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b$, to predict $y$ given $x$.\n",
        "- However, we would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1.\n",
        "- This can be accomplished by using a \"sigmoid function\" which maps all input values to values between 0 and 1.\n",
        "\n",
        "\n",
        "Let's implement the sigmoid function and see this for ourselves.\n",
        "\n",
        "## Formula for Sigmoid function\n",
        "\n",
        "The formula for a sigmoid function is as follows -  \n",
        "\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}\\tag{1}$$\n",
        "\n",
        "In the case of logistic regression, z (the input to the sigmoid function), is the output of a linear regression model.\n",
        "- In the case of a single example, $z$ is scalar.\n",
        "- in the case of multiple examples, $z$ may be a vector consisting of $m$ values, one for each example.\n",
        "- The implementation of the sigmoid function should cover both of these potential input formats.\n",
        "Let's implement this in Python."
      ],
      "metadata": {
        "id": "AFmuRoAxlq_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function\n",
        "\n",
        "In a previous lab, you developed the *logistic loss* function. Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n",
        "\n",
        "\n",
        "Recall that for logistic regression, the cost function is of the form\n",
        "\n",
        "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
        "\n",
        "where\n",
        "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
        "\n",
        "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
        "    \n",
        "*  where m is the number of training examples in the data set and:\n",
        "$$f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) = g(z^{(i)})\\tag{3}$$\n",
        "$$z^{(i)} = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4}$$\n",
        "$$g(z^{(i)}) = \\frac{1}{1+e^{-z^{(i)}}} \\tag{5}$$\n",
        ""
      ],
      "metadata": {
        "id": "HrNtm63Ml5RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Gradient Descent\n",
        "<img align=\"right\" src=\"./C1_W3_Logistic_gradient_descent.png\"     style=\" width:400px; padding: 10px; \" >\n",
        "\n",
        "Recall the gradient descent algorithm utilizes the gradient calculation:\n",
        "$$\\begin{align*}\n",
        "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
        "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\\n",
        "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
        "&\\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "Where each iteration performs simultaneous updates on $w_j$ for all $j$, where\n",
        "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}$$\n",
        "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}$$\n",
        "\n",
        "* m is the number of training examples in the data set      \n",
        "* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n",
        "* For a logistic regression model  \n",
        "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
        "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
        "    where $g(z)$ is the sigmoid function:  \n",
        "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
        "    \n"
      ],
      "metadata": {
        "id": "NmLRHORSmPBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost function for regularized logistic regression\n",
        "For regularized **logistic** regression, the cost function is of the form\n",
        "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{3}$$\n",
        "where:\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = sigmoid(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)  \\tag{4} $$\n",
        "\n",
        "Compare this to the cost function without regularization (which you implemented in  a previous lab):\n",
        "\n",
        "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right] $$\n",
        "\n",
        "As was the case in linear regression above, the difference is the regularization term, which is    <span style=\"color:#00CCFF\">\n",
        "    $\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$ </span>\n",
        "\n",
        "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter $b$ is not regularized. This is standard practice."
      ],
      "metadata": {
        "id": "8439kfR5nyTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent with regularization\n",
        "The basic algorithm for running gradient descent does not change with regularization, it is:\n",
        "$$\\begin{align*}\n",
        "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
        "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\\n",
        "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
        "&\\rbrace\n",
        "\\end{align*}$$\n",
        "Where each iteration performs simultaneous updates on $w_j$ for all $j$.\n",
        "\n",
        "What changes with regularization is computing the gradients."
      ],
      "metadata": {
        "id": "2-30iFaAn2nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing the Gradient with regularization (both linear/logistic)\n",
        "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\\mathbf{w}b}$.\n",
        "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\tag{2}$$\n",
        "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}$$\n",
        "\n",
        "* m is the number of training examples in the data set      \n",
        "* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n",
        "\n",
        "      \n",
        "* For a  <span style=\"color:#00CCFF\"> **linear** </span> regression model  \n",
        "    $f_{\\mathbf{w},b}(x) = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
        "* For a <span style=\"color:#00CCFF\"> **logistic** </span> regression model  \n",
        "    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n",
        "    $f_{\\mathbf{w},b}(x) = g(z)$  \n",
        "    where $g(z)$ is the sigmoid function:  \n",
        "    $g(z) = \\frac{1}{1+e^{-z}}$   \n",
        "    \n",
        "The term which adds regularization is  the <span style=\"color:#00CCFF\">$\\frac{\\lambda}{m} w_j $</span>."
      ],
      "metadata": {
        "id": "8EhEQ9FSn5kJ"
      }
    }
  ]
}